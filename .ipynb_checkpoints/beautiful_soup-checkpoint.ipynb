{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python's BeautifulSoup package\n",
    "\n",
    "BeautifulSoup is a widely used Python package to process and extract element of HTML documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this package to extract the table on the wikipedia page of the List of Largest financial services companies by revenue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_largest_financial_services_companies_by_revenue' \n",
    "r = requests.get(url) \n",
    "print(r.url)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the HTTP request has returned the html document that makes up the wikipedia webpage content. This is messy and the structure of the HTML is not entirely clear at first glance. \n",
    "\n",
    "#### 1. Creating a BeautifulSoup object\n",
    "This is where BeautifulSoup package comes handy! Let's convert the output of request's \"text\" method into a BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_largest_financial_services_companies_by_revenue' \n",
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "\n",
    "if html_content is not None:\n",
    "    # create a beautiful soup object\n",
    "    html_soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    print(type(html_soup))\n",
    "else:\n",
    "    raise Exception('Error getting data from {}'.format(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BeautifulSoup library itself depends on an HTML parser. Python has multiple HTML parsers:\n",
    "- 'html.parser' - Python's built-in parser\n",
    "- 'lxml' - external package, runs very fast\n",
    "- 'html5lib' - aims to parse web page exactly the same way as browser does, is a bit slow\n",
    "\n",
    "#### 2. Methods to extract HTML elements\n",
    "BeautifulSoup takes HTML content and transforms it into a tree-based representation. There are two methods to fetch data from a BeautifulSoup object, which are more commonly used:\n",
    "- find : returns the retrieved element\n",
    "- find_all : return list of the retrieved elements\n",
    "\n",
    "Both methods are used to find elemets inside the HTML tree. You can input the tag name that you wish to find on the page as a string or a list of tags. Next, you can also input attrs argument which takes a Python dictionary of attributes and matches HTML elements that match those attributes. \"find_all\" has an extra argument calles limit which can be used to limit the number of elements that are retreived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup.find('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup.find_all('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Extracting data using attributes of HTML elements\n",
    "\n",
    "Additional attributes can be provided to filter upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup.find('table', {'class': 'wikitable sortable plainrowheads'})\n",
    "# html_soup.find('table', class_= 'wikitable sortable plainrowheads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **keywords search\n",
    "countries = html_soup.find_all(class_= 'datasortkey')\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Filtering the results of find and find_all methods\n",
    "You can select the specific elements from the result of the \"find\" method using the tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_table = html_soup.find('table', {'class': 'wikitable sortable plainrowheads'})\n",
    "print(type(my_table))\n",
    "my_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_table('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eachrow in my_table('tr'):\n",
    "    print('-----------------')\n",
    "    print(eachrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eachrow in my_table('tr'):\n",
    "    print('----------')\n",
    "    print(eachrow('th'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eachrow in my_table('tr'):\n",
    "    print('----------')\n",
    "    print(eachrow(['th','td']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eachrow in my_table('tr')[1:]:\n",
    "    my_data = eachrow(['th','td'])\n",
    "    print('----------')\n",
    "    print(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = html_soup.find_all(class_= 'datasortkey')\n",
    "for i in countries:\n",
    "    img = i.find('img', class_='thumbborder')\n",
    "    country = i.text\n",
    "    print(f\"{country}\\n{img}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in countries:\n",
    "    img = i.find('img', class_='thumbborder')['src']\n",
    "    country = i.text\n",
    "    print(f\"{country}\\n{img}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Extracting data based on string match\n",
    "You can also pass a string to do a look-up under a specific HTML tag and/or attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance = html_soup.find_all('td', string='Insurance')\n",
    "insurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Navigating HTML tree using CSS \n",
    "\n",
    "There is also a \"select\" method that allows us to navigate the html tree based on CSS selectors. Each CSS selectors have HTML attributes that can be accessed like a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup.select('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in html_soup.select('td'):\n",
    "    print(i)\n",
    "    print(i.text)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_table = html_soup.find('table', {'class': 'wikitable sortable plainrowheads'})\n",
    "my_table.select_one('td')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Storing the data\n",
    "\n",
    "Now that we know exactly where the information of rows and columns are stored, we are ready to extract them and store it into dictionary. \n",
    "\n",
    "Let's begin by creating:\n",
    "1. a list of items that will be the columns headers and \n",
    "2. a dictionary who keys are the same column headers and whose values are an empty list, which we will fill with the data we scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the table and convert to Python dictionary\n",
    "mytable_dict = { 'Rank':[], 'Name':[], 'Industry':[], 'Revenue':[], 'NetIncome':[], 'TotalAssets':[], 'Headquarters':[] }\n",
    "\n",
    "for idx, item in enumerate(mytable_dict.keys()):\n",
    "    print(idx, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eachrow in my_table('tr')[1:]:\n",
    "    my_data = eachrow(['th','td'])\n",
    "    \n",
    "    for idx, item in enumerate(mytable_dict.keys()):\n",
    "        mytable_dict[item].append( my_data[idx].text )\n",
    "        print(idx, item)\n",
    "        print(mytable_dict[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the table and convert to Python dictionary\n",
    "mytable_dict = { 'Rank':[], 'Name':[], 'Industry':[], 'Revenue':[], 'NetIncome':[], 'TotalAssets':[], 'Headquarters':[] }\n",
    "\n",
    "for eachrow in my_table('tr')[1:]:\n",
    "    my_data = eachrow(['th','td'])\n",
    "    \n",
    "    for idx, item in enumerate(mytable_dict.keys()):\n",
    "        mytable_dict[item].append( my_data[idx].text.strip() )\n",
    "print(mytable_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.DataFrame(mytable_dict)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an HTTP request and convert the text of response object into beautiful soup object\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_largest_financial_services_companies_by_revenue' \n",
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "\n",
    "if html_content is not None:\n",
    "    html_soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "else:\n",
    "    raise Exception('Error getting data from {}'.format(url))\n",
    "\n",
    "# isolate the table we want and save it into a dataframe\n",
    "my_table = html_soup.find('table', {'class': 'wikitable sortable plainrowheads'})\n",
    "mytable_dict = { 'Rank':[], 'Name':[], 'Industry':[], 'Revenue':[], 'NetIncome':[], 'TotalAssets':[], 'Headquarters':[] }\n",
    "\n",
    "for eachrow in my_table('tr')[1:]:\n",
    "    my_data = eachrow(['th','td'])\n",
    "    \n",
    "    for idx, item in enumerate(mytable_dict.keys()):\n",
    "        mytable_dict[item].append( my_data[idx].text.strip() )\n",
    "        \n",
    "dataframe = pd.DataFrame(mytable_dict)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
