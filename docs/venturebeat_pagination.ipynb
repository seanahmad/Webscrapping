{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping with Pagination\n",
    "\n",
    "You were asked to web scrape the url https://venturebeat.com. Applying what we learned so far, this should be straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GET THE HTML CONTENT OF THE MAIN PAGE\n",
    "def geturlhtml(main_url):\n",
    "    \n",
    "    # make HTTP request\n",
    "    r = requests.get(main_url)\n",
    "    html_content = r.text\n",
    "\n",
    "    # if the request went through and we have some text, \n",
    "    # convert to beautiful object\n",
    "    if html_content is not None:\n",
    "        html_soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    else:\n",
    "        raise Exception('Error getting data from {}'.format(url))\n",
    "        \n",
    "    return html_soup\n",
    "\n",
    "# 2. FROM THE HTML CONTENT OF THE MAIN PAGE GET THE ARTICLE LINKS\n",
    "\n",
    "def getheadlink(main_htmldoc):\n",
    "    return [ main_htmldoc.find('a', class_='Hero__title-link')['href'] ]\n",
    "\n",
    "def getarticlelinks(html_doc):\n",
    "    \n",
    "    article_links = []\n",
    "    links = html_doc.find_all('a', class_='ArticleListing__title-link')\n",
    "    for i in links:\n",
    "        article_links.append(i['href'])\n",
    "#     print(len(links))\n",
    "    return article_links\n",
    "\n",
    "\n",
    "# 3. GET THE HTML CONTENT FOR EACH ARTICLE LINK AND GET THE ARTICLE TITLE AND TEXT\n",
    "def gettextfromarticleurl(article_url):\n",
    "    \n",
    "    # new requests to individual article pages\n",
    "    r = requests.get(article_url)\n",
    "    html_content = r.text\n",
    "    \n",
    "    # convert to beautiful soup object\n",
    "    if html_content is not None:\n",
    "        html_soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    else:\n",
    "        raise Exception('Error getting data from {}'.format(url))\n",
    "    \n",
    "    # grab the category\n",
    "    cat_class = 'Label Label--single Label--brand Label__link--brand'\n",
    "    article_category = html_soup.find(class_=cat_class).text.strip()\n",
    "                                      \n",
    "    # grab the title\n",
    "    article_title = html_soup.find('title').text.strip()\n",
    "    article_title = re.sub(r\" \\|.*$\", \"\", article_title)\n",
    "    \n",
    "    # grab the body\n",
    "    articlecontent = html_soup.find(class_= 'article-content')\n",
    "    article_text = []\n",
    "    for i in articlecontent.find_all('p'):\n",
    "        article_text.append(i.text.strip())\n",
    "    article_text = \" \".join(article_text)\n",
    "    \n",
    "    return article_category, article_title, article_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://venturebeat.com'\n",
    "\n",
    "# 1. GET THE HTML CONTENT OF THE MAIN PAGE\n",
    "html_soup = geturlhtml(url)\n",
    "\n",
    "# 2. FROM THE HTML CONTENT OF THE MAIN PAGE GET THE ARTICLE LINKS\n",
    "article_links = getheadlink(html_soup) + getarticlelinks(html_soup)\n",
    "\n",
    "# 3. GET THE HTML CONTENT FOR EACH ARTICLE LINK AND GET THE ARTICLE TITLE AND TEXT\n",
    "data = [gettextfromarticleurl(i) for i in article_links]\n",
    "article_titles = [i for (i,j) in data]\n",
    "article_texts = [j for (i,j) in data]\n",
    "for i in article_titles:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(article_titles))\n",
    "print(len(article_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Pagination\n",
    "\n",
    "Pagination is a technique in webdesigning that splits content into various pages, thus presenting large datasets in digestible manner for web users. There are many pagination methods:\n",
    "- numbered pagination\n",
    "- infinite scrolling\n",
    "- next button\n",
    "- load more buttons, etc. \n",
    "\n",
    "While pagination makes web browsing experience better, it certainly makes the task of web scrapping more difficult. \n",
    "\n",
    "Let's see an example now. The webpage we are looking to scrape is https://venturebeat.com. When you scroll to the bottom of the page, you will notice that at some point the url changes to https://venturebeat.com/page/2/ and this pattern continues. \n",
    "\n",
    "So, lets repeat what we did above for the url page 2 and see if we get new article links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://venturebeat.com/page/2'\n",
    "\n",
    "# 1. GET THE HTML CONTENT OF THE MAIN PAGE\n",
    "html_soup = geturlhtml(main_url)\n",
    "\n",
    "# 2. FROM THE HTML CONTENT OF THE MAIN PAGE GET THE ARTICLE LINKS\n",
    "article_links2 = getarticlelinks(html_soup)\n",
    "article_links2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply what we have done so far on more than one page i.e. collect article url links from a number of webpages using page numbers. Then, we can run HTTP requests on each of these article links and obtain their corresponding article titles and article contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 1. GET THE HTML CONTENT OF THE MAIN PAGE\n",
    "# page_no = range(2,51,1)\n",
    "# page_urls = []\n",
    "# for i in page_no:\n",
    "#     page_urls.append('https://venturebeat.com/page/'+str(i))\n",
    "\n",
    "# html_soups = [ geturlhtml(url) for url in page_urls ]\n",
    "# html_soups\n",
    "\n",
    "\n",
    "# # 2. FROM THE HTML CONTENT OF THE MAIN PAGE GET THE ARTICLE LINKS\n",
    "# head_link = getheadlink(html_soups[0])\n",
    "# other_links = [ getarticlelinks(html_soup) for html_soup in html_soups ]\n",
    "# other_links = [eachlink \n",
    "#                for eachlist in other_links \n",
    "#                for eachlink in eachlist]\n",
    "# article_links = head_link + other_links\n",
    "\n",
    "\n",
    "# # 3. GET THE HTML CONTENT FOR EACH ARTICLE LINK AND GET THE ARTICLE TITLE AND TEXT FROM IT\n",
    "\n",
    "# data_dictionary = {'url':[], 'category':[], 'title':[], 'text':[]}\n",
    "# tracker = 0\n",
    "# for i in article_links:\n",
    "#     category, title, text = gettextfromarticleurl(i)\n",
    "#     data_dictionary['url'].append(i)\n",
    "#     data_dictionary['category'].append(category)\n",
    "#     data_dictionary['title'].append(title)\n",
    "#     data_dictionary['text'].append(text)\n",
    "#     tracker += 1\n",
    "#     print('processed ', tracker, ' files')\n",
    "    \n",
    "    \n",
    "# print('no of webpages to scrape: ', len(page_urls))\n",
    "# print('header article link: ', head_link)\n",
    "# print('total no. of article links: ', len(article_links))\n",
    "# print('total no. of article titles and texts: ', (len(data_dictionary[title]), \n",
    "#                                                   len(data_dictionary[text])) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1961, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>https://venturebeat.com/2020/03/20/despite-set...</td>\n",
       "      <td>AI</td>\n",
       "      <td>Despite setbacks, coronavirus could hasten the...</td>\n",
       "      <td>This week, nearly every major company developi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>https://venturebeat.com/2020/03/19/sensor-towe...</td>\n",
       "      <td>Games</td>\n",
       "      <td>Sensor Tower: U.S. iPhone users spent about $5...</td>\n",
       "      <td>U.S. iPhone users spent an average of about $5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>https://venturebeat.com/2020/03/19/microsoft-u...</td>\n",
       "      <td>Games</td>\n",
       "      <td>Microsoft unveils DirectX 12 Ultimate with imp...</td>\n",
       "      <td>Microsoft is moving on to the next generation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>https://venturebeat.com/2020/03/19/sea-of-star...</td>\n",
       "      <td>Games</td>\n",
       "      <td>Sea of Stars is a gorgeous retro-RPG from The ...</td>\n",
       "      <td>Sabotage Studios announced Sea of Stars today,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>https://venturebeat.com/2020/03/19/htc-holds-v...</td>\n",
       "      <td>AR/VR</td>\n",
       "      <td>HTC holds virtual media event, sends coronavir...</td>\n",
       "      <td>HTC’s just-concluded Virtual Vive Ecosystem Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url category  \\\n",
       "0  https://venturebeat.com/2020/03/20/despite-set...       AI   \n",
       "1  https://venturebeat.com/2020/03/19/sensor-towe...    Games   \n",
       "2  https://venturebeat.com/2020/03/19/microsoft-u...    Games   \n",
       "3  https://venturebeat.com/2020/03/19/sea-of-star...    Games   \n",
       "4  https://venturebeat.com/2020/03/19/htc-holds-v...    AR/VR   \n",
       "\n",
       "                                               title  \\\n",
       "0  Despite setbacks, coronavirus could hasten the...   \n",
       "1  Sensor Tower: U.S. iPhone users spent about $5...   \n",
       "2  Microsoft unveils DirectX 12 Ultimate with imp...   \n",
       "3  Sea of Stars is a gorgeous retro-RPG from The ...   \n",
       "4  HTC holds virtual media event, sends coronavir...   \n",
       "\n",
       "                                                text  \n",
       "0  This week, nearly every major company developi...  \n",
       "1  U.S. iPhone users spent an average of about $5...  \n",
       "2  Microsoft is moving on to the next generation ...  \n",
       "3  Sabotage Studios announced Sea of Stars today,...  \n",
       "4  HTC’s just-concluded Virtual Vive Ecosystem Co...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data_dictionary)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://venturebeat.com/2020/03/20/despite-setbacks-coronavirus-could-hasten-the-adoption-of-autonomous-vehicles-and-delivery-robots/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Despite setbacks, coronavirus could hasten the adoption of autonomous vehicles and delivery robots'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1961 entries, 0 to 1960\n",
      "Data columns (total 4 columns):\n",
      "url         1961 non-null object\n",
      "category    1961 non-null object\n",
      "title       1961 non-null object\n",
      "text        1961 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 61.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('venturebeat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
